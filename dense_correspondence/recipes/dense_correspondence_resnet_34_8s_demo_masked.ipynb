{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '../pytorch-segmentation-detection/vision/')\n",
    "sys.path.append('../pytorch-segmentation-detection/')\n",
    "\n",
    "# Use second GPU -pytorch-segmentation-detection- change if you want to use a first one\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import pytorch_segmentation_detection.models.resnet_dilated as resnet_dilated\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import sys; sys.path.append('./dataset')\n",
    "import correspondence_plotter\n",
    "from labelfusion import LabelFusionDataset\n",
    "\n",
    "\n",
    "descriptor_dimensionality = 3\n",
    "nets = sorted(glob.glob(\"trained_models/\"+str(descriptor_dimensionality)+\"d/dense_resnet*.pth\"))\n",
    "print \"Networks:\"\n",
    "for net in nets:\n",
    "    print \"   - \", net\n",
    "\n",
    "lf = LabelFusionDataset()\n",
    "scene = \"2017-06-13-12\"\n",
    "\n",
    "img_a_index = \"0000000001\"\n",
    "img_a_rgb, img_a_depth, img_a_pose = lf.get_specific_rgbd_with_pose(scene, img_a_index)\n",
    "\n",
    "img_b_index = \"0000000300\"\n",
    "img_b_rgb, img_b_depth, img_b_pose = lf.get_specific_rgbd_with_pose(scene, img_b_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose(\n",
    "                [\n",
    "                     transforms.ToTensor(),\n",
    "                ])\n",
    "\n",
    "def forward_on_img(net, img):\n",
    "    img = valid_transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = Variable(img.cuda())\n",
    "    fcn = resnet_dilated.Resnet34_8s(num_classes=descriptor_dimensionality)\n",
    "    fcn.load_state_dict(torch.load(net))\n",
    "    fcn.cuda()\n",
    "    fcn.eval()\n",
    "    res = fcn(img)\n",
    "    res = res.squeeze(0)\n",
    "    res = res.permute(1,2,0)\n",
    "    res = res.data.cpu().numpy().squeeze()\n",
    "    return res\n",
    "\n",
    "res_a = forward_on_img(nets[0], img_a_rgb)\n",
    "res_b = forward_on_img(nets[0], img_b_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(res):\n",
    "    normed_res = res + -np.min(res)\n",
    "    normed_res = normed_res / np.max(normed_res)\n",
    "    return normed_res\n",
    "\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    axes = axes.flat[0:]\n",
    "    images = [img_a_rgb, img_b_rgb, img_a_depth, img_b_depth, normalize(res_a), normalize(res_b)]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do final desciprtors look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    for index, this_net in enumerate(nets[-1:]):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "        fig.set_figheight(5)\n",
    "        fig.set_figwidth(15)\n",
    "        if index == 0:\n",
    "            axes[0].set_title(\"img a\")\n",
    "            axes[1].set_title(\"img b\")\n",
    "        axes[0].imshow(normalize(forward_on_img(this_net, img_a_rgb)))\n",
    "        axes[1].imshow(normalize(forward_on_img(this_net, img_b_rgb)))\n",
    "        axes[0].set_ylabel(this_net)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about overlaying these images?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_net = nets[-1]\n",
    "if (descriptor_dimensionality == 1) or (descriptor_dimensionality == 3):\n",
    "    fig, axes = plt.subplots(1, ncols=2)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    axes[0].set_title(\"img a\")\n",
    "    axes[1].set_title(\"img b\")\n",
    "    axes[0].imshow(normalize(forward_on_img(this_net, img_a_rgb)))\n",
    "    axes[0].imshow(img_a_rgb, alpha=0.5)\n",
    "    axes[1].imshow(normalize(forward_on_img(this_net, img_b_rgb)))\n",
    "    axes[1].imshow(img_b_rgb, alpha=0.5)\n",
    "    axes[0].set_ylabel(this_net)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we use the mask from LabelFusion data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: plot a mask\n",
    "scene_dir = lf.get_full_path_for_scene(\"2017-06-13-12\")\n",
    "images_dir = os.path.join(scene_dir, \"images\")\n",
    "\n",
    "mask_a_color_file = images_dir+\"/\"+img_a_index+\"_color_labels.png\"\n",
    "mask_a_color = Image.open(mask_a_color_file).convert('RGB')\n",
    "mask_a_file = images_dir+\"/\"+img_a_index+\"_labels.png\"\n",
    "mask_a = np.asarray(Image.open(mask_a_file))\n",
    "\n",
    "\n",
    "mask_b_color_file = images_dir+\"/\"+img_b_index+\"_color_labels.png\"\n",
    "mask_b_color = Image.open(mask_b_color_file).convert('RGB')\n",
    "mask_b_file = images_dir+\"/\"+img_b_index+\"_labels.png\"\n",
    "mask_b = np.asarray(Image.open(mask_b_file))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "axes[0,0].set_title(\"img a\")\n",
    "axes[0,1].set_title(\"img b\")\n",
    "axes[0,0].imshow(img_a_rgb)\n",
    "axes[0,0].imshow(mask_a, alpha=0.5)\n",
    "axes[0,1].imshow(img_b_rgb)\n",
    "axes[0,1].imshow(mask_b, alpha=0.5)\n",
    "axes[1,0].imshow(mask_a_color)\n",
    "axes[1,1].imshow(mask_b_color)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we mask the dense descriptor representation for the drill?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "mask_a_3_channel = np.zeros((480,640,3))\n",
    "mask_a_3_channel[:,:,0] = mask_a_3_channel[:,:,1] = mask_a_3_channel[:,:,2] = mask_a/np.max(mask_a)\n",
    "mask_b_3_channel = np.zeros((480,640,3))\n",
    "mask_b_3_channel[:,:,0] = mask_b_3_channel[:,:,1] = mask_b_3_channel[:,:,2] = mask_b/np.max(mask_a)\n",
    "\n",
    "masked_res_a = mask_a_3_channel*normalize(res_a)\n",
    "masked_res_b = mask_b_3_channel*normalize(res_b)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "axes[0].set_title(\"img a\")\n",
    "axes[1].set_title(\"img b\")\n",
    "axes[0].imshow(masked_res_a)\n",
    "axes[1].imshow(masked_res_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we multiply each pixel by some scalar?\n",
    "\n",
    "Note: this will implicitly cast each float to a uint8, and we'll get some psuedo-normalization with wrapping to help differentiate different colors better. See: \n",
    "https://stackoverflow.com/questions/39925420/bizzare-matplotlib-behaviour-in-displaying-images-cast-as-floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_a = forward_on_img(this_net, img_a_rgb)\n",
    "res_b = forward_on_img(this_net, img_b_rgb)\n",
    "\n",
    "mask_a_3_channel = np.zeros((480,640,3))\n",
    "mask_a_3_channel[:,:,0] = mask_a_3_channel[:,:,1] = mask_a_3_channel[:,:,2] = mask_a/np.max(mask_a)\n",
    "mask_b_3_channel = np.zeros((480,640,3))\n",
    "mask_b_3_channel[:,:,0] = mask_b_3_channel[:,:,1] = mask_b_3_channel[:,:,2] = mask_b/np.max(mask_a)\n",
    "\n",
    "normalized_masked_res_a = normalize(mask_a_3_channel*res_a)\n",
    "normalized_masked_res_b = normalize(mask_b_3_channel*res_b)\n",
    "\n",
    "scale = 6\n",
    "\n",
    "masked_res_a = mask_a_3_channel*normalized_masked_res_a*scale\n",
    "masked_res_b = mask_b_3_channel*normalized_masked_res_b*scale\n",
    "\n",
    "print np.max(masked_res_a)\n",
    "#print masked_res_a[135:145,340:350]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "axes[0].set_title(\"img a\")\n",
    "axes[1].set_title(\"img b\")\n",
    "axes[0].imshow(masked_res_a)\n",
    "axes[1].imshow(masked_res_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
